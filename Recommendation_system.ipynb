{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOckASBvoiYhwwAaquHUAPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paoins/Job-Recommendation/blob/main/Recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuRdABc38W_m"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"asaniczka/1-3m-linkedin-jobs-and-skills-2024\")\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "dataset_src = path\n",
        "dataset_working = '/content/dataset'\n",
        "\n",
        "# Remove if exists\n",
        "shutil.rmtree(dataset_working, ignore_errors=True)\n",
        "\n",
        "shutil.copytree(dataset_src, dataset_working)\n",
        "print(\"Dataset copied to /content/dataset\")\n"
      ],
      "metadata": {
        "id": "89SRjZ6ae7X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview"
      ],
      "metadata": {
        "id": "ukGXSej1EJ21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Are We Building?\n",
        "- A job recommendation system that suggests relevant data science jobs to candidates based on:\n",
        "\n",
        "  - Their skills\n",
        "  - Experience level\n",
        "  - Location preferences\n",
        "  - Similar candidates' behavior (collaborative filtering)\n"
      ],
      "metadata": {
        "id": "XPxYrPpuEMAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why This Approach?\n",
        "- **Real-world problem:** Job boards show thousands of irrelevant jobs. We want to rank jobs by relevance.\n",
        "\n",
        "- **Our solution:** Three recommendation approaches:\n",
        "\n",
        "    1. **Content-Based:** Match based on skills/experience (like a smart filter)\n",
        "    2. **Collaborative Filtering:** Learn from patterns (like \"people who liked X also liked Y\")\n",
        "    3. **Hybrid:** Combine both for best results\n"
      ],
      "metadata": {
        "id": "fIchWiAIEaWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.DATA LOADING & PREPROCESSING\n",
        "Loads 1.3 million LinkedIn jobs, filters for data science roles, cleans the data, and prepares it for modeling.\n"
      ],
      "metadata": {
        "id": "9aHHNi_G9m1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import ast\n",
        "import random\n",
        "from scipy.sparse import csr_matrix\n"
      ],
      "metadata": {
        "id": "ZO060tdX9qQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We loads only specific columns from the CSV Why: The dataset has 30+ columns; we only need 8 to save memory\n",
        "\n",
        "**Columns explained:**\n",
        "\n",
        " - **job_link**: Unique identifier for each job\n",
        "- **job_title**: \"Senior Data Scientist\", \"ML Engineer\", etc.\n",
        "- **company**: \"Google\", \"Meta\", etc.\n",
        "- **job_location**: \"San Francisco, CA\" or \"Remote\"\n",
        "- **got_summary**: Boolean : does this job have a description?\n",
        "- **got_ner**: Boolean = was Named Entity Recognition applied?\n"
      ],
      "metadata": {
        "id": "UJvbWjDoE522"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Job Posting\n",
        "usecols = [\n",
        "            'job_link', 'job_title', 'company', 'job_location',\n",
        "            'first_seen', 'search_city', 'got_summary', 'got_ner'\n",
        "        ]\n",
        "\n",
        "job_postings = pd.read_csv(\n",
        "            \"/content/dataset/linkedin_job_postings.csv\",\n",
        "            usecols=usecols\n",
        "        )"
      ],
      "metadata": {
        "id": "AzbN2eG49zB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Loaded {len(job_postings):,} job postings\")\n"
      ],
      "metadata": {
        "id": "_Iwa2C0Y-Eqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load job Job summaries (full descriptions) and Job skills (required skills like \"Python\", \"SQL\")\n"
      ],
      "metadata": {
        "id": "khSzlTHiFTV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Job Summaries\n",
        "job_summaries = pd.read_csv(\n",
        "            \"/content/dataset/job_summary.csv\"\n",
        "        )\n",
        "print(f\"Loaded {len(job_summaries):,} job summaries\")\n",
        "\n"
      ],
      "metadata": {
        "id": "myHjhbH7-ylo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading skills\n",
        "job_skills = pd.read_csv(\n",
        "            \"/content/dataset/job_skills.csv\"\n",
        "        )\n",
        "print(f'Loaded {len(job_skills):,} job skills')"
      ],
      "metadata": {
        "id": "xzB4W-68DiiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Check\n",
        "print(f\"\\nData Quality Check:\")\n",
        "print(f\"  - Missing job titles: {job_postings['job_title'].isna().sum()}\")\n",
        "print(f\"  - Missing locations: {job_postings['job_location'].isna().sum()}\")\n",
        "print(f\"  - Duplicate job links: {job_postings['job_link'].duplicated().sum()}\")\n"
      ],
      "metadata": {
        "id": "1oO9HgspVY2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Combines all 3 datasets using `job_link` as the key"
      ],
      "metadata": {
        "id": "P48NWu0sGaC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the 3 datasets\n",
        "\n",
        "# Merge postings with summaries\n",
        "merged = job_postings.merge(job_summaries, on='job_link', how='left')\n",
        "print(f\"After merging summaries: {len(merged):,} rows\")\n",
        "\n",
        "# Merge with skills\n",
        "df = merged.merge(job_skills, on='job_link', how='left')\n",
        "print(f\"After merging skills: {len(df):,} rows\")\n"
      ],
      "metadata": {
        "id": "4te2n6gGD_sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `parse_skills`: Convert skills to list\n",
        "- `extract_experience_level`: Extracts experience level from job title"
      ],
      "metadata": {
        "id": "RWBmHGHPGicA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split , Strip , Lower and strip\n",
        "def parse_skills(skills):\n",
        "    if pd.isna(skills):\n",
        "        return []\n",
        "    return [s.strip().lower() for s in skills.split(',') if s.strip()]\n",
        "\n",
        "# Infers seniority from Title\n",
        "def extract_experience_level(title):\n",
        "  # Lower\n",
        "    title = str(title).lower()\n",
        "    if any(x in title for x in ['senior', 'sr', 'lead', 'principal']):\n",
        "        return 'Senior'\n",
        "    if any(x in title for x in ['junior', 'jr', 'entry', 'graduate', 'intern']):\n",
        "        return 'Junior'\n",
        "    if any(x in title for x in ['manager', 'head', 'director']):\n",
        "        return 'Manager'\n",
        "    return 'Mid' # If no keyword assume mid"
      ],
      "metadata": {
        "id": "ajfERoJZHkGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Removes jobs without skills or summaries\n",
        "- Creates new columns\n",
        "  - `skills_list`: Parsed skills (list format)\n",
        "  - `experience_level`: Extracted from title\n",
        "  - `job_id`: Unique ID like 'JOB_000001', 'JOB_000002'\n"
      ],
      "metadata": {
        "id": "2qsVKaJyGlv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Track the loss\n",
        "initial_rows = len(df)\n",
        "\n",
        "# Remove the job without skills\n",
        "df = df[df['job_skills'].notna()].copy()\n",
        "print(f\"Removed {initial_rows - len(df):,} jobs without skills\")\n",
        "\n",
        "# Remove jobs without summaries\n",
        "df = df[df['job_summary'].notna()].copy()\n",
        "print(f\"Kept {len(df):,} jobs with both skills and summaries\")\n",
        "\n",
        "# Handle missing locations safely\n",
        "df['job_location'] = df['job_location'].fillna('Remote')\n",
        "\n",
        "# Parse skills into a usable format\n",
        "df['skills_list'] = df['job_skills'].apply(parse_skills)\n",
        "\n",
        "# Infer experience level from title\n",
        "df['experience_level'] = df['job_title'].apply(extract_experience_level)\n",
        "\n",
        "# Generate stable job IDs\n",
        "df['job_id'] = ['JOB_' + str(i).zfill(6) for i in range(len(df))]"
      ],
      "metadata": {
        "id": "-AYeo2n-EUfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Filters for only data science jobs"
      ],
      "metadata": {
        "id": "JjqMYY8CH83z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for data science related jobs\n",
        "\n",
        "# Keywords for data science jobs\n",
        "ds_keywords = [\n",
        "            'data scientist', 'data science', 'machine learning', 'ml engineer',\n",
        "            'data analyst', 'data engineer', 'ai engineer', 'analytics',\n",
        "            'business intelligence', 'deep learning', 'nlp engineer',\n",
        "            'research scientist', 'applied scientist'\n",
        "        ]\n",
        "\n",
        "# Filter based on job title\n",
        "mask = df['job_title'].str.lower().str.contains('|'.join(ds_keywords), na=False)\n",
        "filtered_df = df[mask].copy()\n",
        "\n",
        "print(f\" Found {len(filtered_df):,} data science related jobs\")\n",
        "\n"
      ],
      "metadata": {
        "id": "efsIU0Lgqcal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Ensure the directory exists\n",
        "output_dir = Path(\"data/processed\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Save the dataset to CSV\n",
        "output_path = output_dir / \"data_science_jobs.csv\"\n",
        "filtered_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\" Saved {len(filtered_df):,} Data Science jobs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "hjJefnKzUSW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten all skills\n",
        "all_skills = []\n",
        "for skills_list in filtered_df['skills_list']:\n",
        "            all_skills.extend(skills_list)\n",
        "\n",
        "# Count occurrences\n",
        "skill_counts = pd.Series(all_skills).value_counts()\n",
        "\n",
        "print(f\"Found {len(skill_counts):,} unique skills\")\n",
        "print(f\"\\nTop 20 most common skills:\")\n",
        "print(skill_counts.head(20))"
      ],
      "metadata": {
        "id": "IhlCCvPqGLvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Candidate\n",
        "- Creates 1,000 realistic fake candidate profiles with skills, experience, education, and salary expectations.\n",
        "\n",
        "- **Problem**: We don't have real candidate data (privacy concerns)\n",
        "\n",
        "* **Solution**: Simulate realistic candidates so we can:\n",
        "\n",
        "    - Test the recommendation system\n",
        "    - Train the collaborative filtering model\n",
        "    - Show the app working with sample data\n"
      ],
      "metadata": {
        "id": "Uy7pU6udd9a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We loop through every job's skill list\n",
        "2. Add all skills to one big list (with duplicates)\n",
        "3. `set()` removes duplicates to keep unique skills\n",
        "4. Count how often each skill appears"
      ],
      "metadata": {
        "id": "KUhEyFecI3Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Unique skills from job data\n",
        "def extract_skill_pool(job_skills_df):\n",
        "    all_skills = []\n",
        "    for skills_list in job_skills_df['skills_list']:\n",
        "        all_skills.extend(skills_list)\n",
        "\n",
        "    skill_pool = list(set(all_skills))\n",
        "    skill_frequencies = pd.Series(all_skills).value_counts()\n",
        "\n",
        "    print(f\" Extracted {len(skill_pool)} unique skills from job data\")\n",
        "    return skill_pool, skill_frequencies\n"
      ],
      "metadata": {
        "id": "rM3NoDmWeGfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8AsQO7N0JGge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_candidate(idx, skill_pool, skill_frequencies):\n",
        "\n",
        "    # Randomly assigns years of experience (0-15) with realistic distribution\n",
        "    weights = np.array([0.10, 0.10, 0.10, 0.10, 0.10, 0.10,\n",
        "                        0.07, 0.07, 0.07, 0.07, 0.07,\n",
        "                        0.03, 0.03, 0.03, 0.03, 0.03])\n",
        "    experience_years = np.random.choice(range(16), p=weights / weights.sum())\n",
        "\n",
        "    # Maps years of experience to Experience Level and Number of skills\n",
        "    if experience_years == 0:\n",
        "        exp_level, n_skills = 'Internship', np.random.randint(3, 8)\n",
        "    elif experience_years <= 2:\n",
        "        exp_level, n_skills = 'Entry Level', np.random.randint(5, 12)\n",
        "    elif experience_years <= 5:\n",
        "        exp_level, n_skills = 'Mid Level', np.random.randint(8, 18)\n",
        "    elif experience_years <= 10:\n",
        "        exp_level, n_skills = 'Senior', np.random.randint(12, 25)\n",
        "    else:\n",
        "        exp_level, n_skills = 'Leadership', np.random.randint(15, 30)\n",
        "\n",
        "    # Gives each candidate a \"specialization\" and not just random skills\n",
        "    SKILL_CLUSTERS = {\n",
        "        \"ML\": [\"machine learning\", \"deep learning\", \"pytorch\", \"tensorflow\", \"nlp\", \"computer vision\"],\n",
        "        \"Data\": [\"python\", \"sql\", \"pandas\", \"numpy\", \"data analysis\", \"statistics\"],\n",
        "        \"DE\": [\"spark\", \"hadoop\", \"airflow\", \"aws\", \"gcp\", \"etl\", \"data engineering\"],\n",
        "        \"BI\": [\"tableau\", \"power bi\", \"excel\", \"data visualization\", \"business intelligence\"],\n",
        "        \"Backend\": [\"java\", \"scala\", \"apis\", \"microservices\"]\n",
        "    }\n",
        "\n",
        "    cluster = random.choice(list(SKILL_CLUSTERS.keys()))\n",
        "    cluster_skills = [s for s in SKILL_CLUSTERS[cluster] if s in skill_pool]\n",
        "\n",
        "    top_skills = skill_frequencies.head(200).index.tolist()\n",
        "    other_skills = list(set(skill_pool) - set(cluster_skills))\n",
        "\n",
        "    # Picks skills for the candidate ( 60% from their specialization cluster + 40% from other areas)\n",
        "    n_cluster = max(1, int(n_skills * 0.6))\n",
        "    n_other = n_skills - n_cluster\n",
        "    selected_skills = (\n",
        "        random.sample(cluster_skills, min(len(cluster_skills), n_cluster)) +\n",
        "        random.sample(other_skills, min(len(other_skills), n_skills - n_cluster))\n",
        "    )\n",
        "\n",
        "    selected_skills = list(set(selected_skills))[:n_skills]\n",
        "\n",
        "    # Assigns education based on experience level\n",
        "    EDU_BY_LEVEL = {\n",
        "        'Internship': [\"Bachelor's in Computer Science\", \"Bachelor's in Data Science\"],\n",
        "        'Entry Level': [\"Bachelor's in Computer Science\", \"Bachelor's in Data Science\"],\n",
        "        'Mid Level': [\"Master's in Computer Science\", \"Bachelor's in Engineering\"],\n",
        "        'Senior': [\"Master's in Computer Science\", \"Master's in Applied Mathematics\"],\n",
        "        'Leadership': [\"Master's in Computer Science\", \"PhD in Machine Learning\"]\n",
        "    }\n",
        "\n",
        "    # Set salary exp\n",
        "    salary_ranges = {\n",
        "        'Internship': (40000, 80000),\n",
        "        'Entry Level': (60000, 100000),\n",
        "        'Mid Level': (90000, 150000),\n",
        "        'Senior': (130000, 200000),\n",
        "        'Leadership': (160000, 280000)\n",
        "    }\n",
        "    min_sal, max_sal = salary_ranges[exp_level]\n",
        "\n",
        "    locations = [\n",
        "        'Remote', 'New York, NY', 'San Francisco, CA', 'Seattle, WA',\n",
        "        'Austin, TX', 'Boston, MA', 'Chicago, IL'\n",
        "    ]\n",
        "\n",
        "    # Randomly picks 1-3 preferred cities\n",
        "    preferred_locations = random.sample(locations, random.randint(1, 3))\n",
        "    salary_min = np.random.randint(min_sal, max_sal - 20000)\n",
        "\n",
        "    return {\n",
        "        'candidate_id': f'CAND_{idx+1:06d}',\n",
        "        'experience_years': int(experience_years),\n",
        "        'experience_level': exp_level,\n",
        "        'domain': cluster,\n",
        "        'skills_list': selected_skills,\n",
        "        'skills': ', '.join(selected_skills),\n",
        "        'education': random.choice(EDU_BY_LEVEL[exp_level]),\n",
        "        'desired_salary_min': int(salary_min),\n",
        "        'desired_salary_max': int(salary_min + np.random.randint(20000, 40000)),\n",
        "        'preferred_locations': ', '.join(preferred_locations),\n",
        "        'open_to_remote': 'Remote' in preferred_locations,\n",
        "        'willing_to_relocate': random.random() > 0.5\n",
        "    }"
      ],
      "metadata": {
        "id": "PGcJQ0xaeZgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(skill_pool, skill_frequencies, n_candidates=1000):\n",
        "    print(f\"\\n Generating {n_candidates} candidate profiles...\")\n",
        "    candidates = []\n",
        "\n",
        "    for i in range(n_candidates):\n",
        "        candidates.append(\n",
        "            create_single_candidate(i, skill_pool, skill_frequencies)\n",
        "        )\n",
        "\n",
        "        if (i + 1) % 200 == 0:\n",
        "            print(f\"   Generated {i+1}/{n_candidates} candidates...\")\n",
        "\n",
        "    df = pd.DataFrame(candidates)\n",
        "    print(f\" Generated {len(df)} candidates\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "auzOf90ZenCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_candidates(df, filename=\"candidates.csv\"):\n",
        "    output_dir = Path(\"data/processed\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    output_path = output_dir / filename\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n Saved {len(df)} candidates to {output_path}\")\n"
      ],
      "metadata": {
        "id": "j3_-a1I2e8G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"   Candidate Profile Generation \")\n",
        "print(\"=\"*60)\n",
        "\n",
        "skill_pool, skill_frequencies = extract_skill_pool(filtered_df)\n",
        "candidates_df = generate_candidates(skill_pool, skill_frequencies, n_candidates=1000)\n",
        "save_candidates(candidates_df)\n"
      ],
      "metadata": {
        "id": "mXuVI4VWfGAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering\n",
        "- Converts text and categorical data into numbers that machine learning algorithms can understand."
      ],
      "metadata": {
        "id": "0p1X-MJuwG2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.head()"
      ],
      "metadata": {
        "id": "rLUGGzEe8juH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans up skill names\n",
        "def clean_skill(skill):\n",
        "    skill = skill.lower().strip()\n",
        "    if any(char.isdigit() for char in skill):\n",
        "        return None\n",
        "    if re.search(r'[\\$\\+\\*\\(\\)]', skill):\n",
        "        return None\n",
        "    if len(skill.split()) > 3:\n",
        "        return None\n",
        "    return skill\n"
      ],
      "metadata": {
        "id": "3FjydOtvN3yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df['skills_list'] = filtered_df['skills_list'].apply(\n",
        "    lambda skills: [clean_skill(s) for s in skills if clean_skill(s)]\n",
        ")"
      ],
      "metadata": {
        "id": "dXdz-UX6N6dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix for jobs\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gc\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "skill_encoder = MultiLabelBinarizer()\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=500,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2\n",
        "  )\n",
        "\n",
        "# 1. Clear memory from the previous crash\n",
        "gc.collect()\n",
        "\n",
        "# 2. Identify the Top 1000 skills\n",
        "all_skills_flat = [s for sublist in filtered_df['skills_list'] for s in sublist]\n",
        "top_1000_skills = pd.Series(all_skills_flat).value_counts().head(1000).index.tolist()\n",
        "\n",
        "print(f\" Selected top 1000 skills out of {len(set(all_skills_flat))} total\")\n",
        "\n",
        "# 3. Create the encoder using ONLY those 1000 skills\n",
        "skill_encoder = MultiLabelBinarizer(classes=top_1000_skills)\n",
        "\n",
        "print(\"Encoding skills...\")\n",
        "skill_matrix = skill_encoder.fit_transform(filtered_df['skills_list'])\n",
        "skill_feature_names = top_1000_skills\n",
        "\n",
        "# 4. Process Text, Experience, and Remote\n",
        "print(\"Vectorizing job descriptions...\")\n",
        "summaries = filtered_df['job_summary'].fillna('')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(summaries)\n",
        "\n",
        "print(\"Encoding experience and location...\")\n",
        "exp_level_dummies = pd.get_dummies(filtered_df['experience_level'], prefix='exp')\n",
        "filtered_df['is_remote'] = filtered_df['job_location'].str.contains('remote', case=False, na=False).astype(int)\n",
        "\n",
        "# 5. Combine everything\n",
        "print(\"Combining features...\")\n",
        "\n",
        "skill_df = pd.DataFrame(\n",
        "    skill_matrix,\n",
        "    columns=[f'skill_{s}' for s in skill_feature_names]\n",
        ")\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]\n",
        ")\n",
        "\n",
        "job_feature_df = pd.concat([\n",
        "    filtered_df[['job_id', 'is_remote']].reset_index(drop=True),\n",
        "    exp_level_dummies.reset_index(drop=True),\n",
        "    skill_df,\n",
        "    tfidf_df\n",
        "], axis=1)\n",
        "job_feature_df.to_csv(\"data/processed/job_features.csv\", index=False)\n",
        "\n",
        "print(f\" Created feature matrix: {job_feature_df.shape}\")"
      ],
      "metadata": {
        "id": "CJtXdEe7wEm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix for candidates using fitted encoders\n",
        "candidates_df= pd.read_csv(\"data/processed/candidates.csv\")\n",
        "candidates_df['skills_list'] = candidates_df['skills_list'].apply(ast.literal_eval)\n",
        "\n",
        "print(f\"Loaded {len(candidates_df)} candidates\")\n",
        "print(f\"Sample skills: {type(candidates_df['skills_list'].iloc[0])}\")\n",
        "\n",
        "# Validation\n",
        "print(\"\\n  Data Validation:\")\n",
        "print(f\"  Skills list type: {type(candidates_df['skills_list'].iloc[0])}\")\n",
        "print(f\"  Sample skills: {candidates_df['skills_list'].iloc[0][:3]}\")\n",
        "\n",
        "assert isinstance(candidates_df['skills_list'].iloc[0], list), \\\n",
        "    \"ERROR: skills_list should be a list, not a string!\"\n",
        "\n",
        "\n",
        "# 1. Skill-based features (using same encoder as jobs)\n",
        "print(\"   Encoding candidate skills...\")\n",
        "skill_matrix = skill_encoder.transform(candidates_df['skills_list'])\n",
        "skill_feature_names = skill_encoder.classes_\n",
        "\n",
        "# 2. Experience level encoding\n",
        "print(\"   Encoding experience levels...\")\n",
        "exp_level_dummies = pd.get_dummies(\n",
        "            candidates_df['experience_level'],\n",
        "            prefix='exp'\n",
        ")\n",
        "\n",
        "       # Ensure same columns as jobs\n",
        "for col in ['exp_Entry Level', 'exp_Internship', 'exp_Leadership',\n",
        "                    'exp_Mid Level', 'exp_Senior']:\n",
        "                    if col not in exp_level_dummies.columns:\n",
        "                      exp_level_dummies[col] = 0\n",
        "\n",
        "# 3. Remote preference\n",
        "print(\"   Processing preferences...\")\n",
        "candidates_df['is_remote'] = candidates_df['open_to_remote'].astype(int)\n",
        "\n",
        "        # Combine features\n",
        "skill_df = pd.DataFrame(\n",
        "            skill_matrix,\n",
        "            columns=[f'skill_{s}' for s in skill_feature_names]\n",
        "        )\n",
        "\n",
        "\n",
        "candidate_feature_df = pd.concat([\n",
        "            candidates_df[['candidate_id', 'is_remote']].reset_index(drop=True),\n",
        "            exp_level_dummies.reset_index(drop=True),\n",
        "            skill_df\n",
        "        ], axis=1)\n",
        "\n",
        "candidate_feature_df.to_csv(\"data/processed/candidate_features.csv\", index=False)\n",
        "print(f\"Created candidate feature matrix: {candidate_feature_df.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YXnIgjBwzGow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Feature Summary:\")\n",
        "print(f\"  Total features: {candidate_feature_df.shape[1]}\")\n",
        "print(f\"  - Skill features: {skill_matrix.shape[1]}\")\n",
        "print(f\"  - Experience features: {len(exp_level_dummies.columns)}\")"
      ],
      "metadata": {
        "id": "I0DpKqJ_tbBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Content_based Recomender\n",
        "- Recommends jobs by directly comparing candidate profile to job requirements"
      ],
      "metadata": {
        "id": "Ubr9U3L493bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates how well candidate's skills match job requirements\n",
        "def calculate_skill_match_score(candidate_skills, job_skills):\n",
        "    candidate_set = set(candidate_skills)\n",
        "    job_set = set(job_skills)\n",
        "    if not job_set: return 0.0\n",
        "\n",
        "    intersection = len(candidate_set & job_set)\n",
        "    union = len(candidate_set | job_set)\n",
        "\n",
        "    # jaccard = (skills in common) / (all unique skills)\n",
        "    jaccard_score = intersection / union if union > 0 else 0\n",
        "\n",
        "    # coverage = (skills you have that job needs) / (total skills job needs)\n",
        "    coverage_score = intersection / len(job_set) if len(job_set) > 0 else 0\n",
        "\n",
        "    return (0.6 * coverage_score) + (0.4 * jaccard_score)\n",
        "\n",
        "\n",
        "# Scores how well candidate's experience matches job\n",
        "def calculate_experience_match(candidate_exp, job_exp):\n",
        "    hierarchy = {'Internship': 0, 'Entry Level': 1, 'Mid Level': 2, 'Senior': 3, 'Leadership': 4}\n",
        "    cand_level = hierarchy.get(candidate_exp, 2)\n",
        "    job_level = hierarchy.get(job_exp, 2)\n",
        "    diff = abs(cand_level - job_level)\n",
        "\n",
        "    mapping = {0: 1.0, 1: 0.7, 2: 0.4}\n",
        "    return mapping.get(diff, 0.2)\n",
        "\n",
        "# Scores location compatibility\n",
        "def calculate_location_match(candidate_locations, job_location, candidate_remote):\n",
        "    if pd.isna(job_location): return 0.5\n",
        "    job_loc = str(job_location).lower()\n",
        "\n",
        "    if 'remote' in job_loc and candidate_remote: return 1.0\n",
        "\n",
        "    candidate_locs = [loc.strip().lower() for loc in str(candidate_locations).split(',')]\n",
        "    for loc in candidate_locs:\n",
        "        if loc in job_loc or job_loc in loc: return 1.0\n",
        "\n",
        "    return 0.5 if candidate_remote else 0.3"
      ],
      "metadata": {
        "id": "DNgxyFKe94fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_salary_match(candidate_salary, job_salary=None):\n",
        "    if job_salary is None:\n",
        "        return 0.7\n",
        "    return 1.0 if candidate_salary <= job_salary else 0.4"
      ],
      "metadata": {
        "id": "uMdL6gtPPCz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(candidate_id, df_jobs, df_candidates, top_k=5):\n",
        "    # Validate candidate exists\n",
        "    candidate_match = df_candidates[df_candidates['candidate_id'] == candidate_id]\n",
        "    if len(candidate_match) == 0:\n",
        "        raise ValueError(f\"Candidate {candidate_id} not found!\")\n",
        "\n",
        "    candidate = candidate_match.iloc[0]\n",
        "    results =[]\n",
        "\n",
        "    for _, job in df_jobs.iterrows():\n",
        "        # Get individual scores\n",
        "        s_score = calculate_skill_match_score(candidate['skills_list'], job['skills_list'])\n",
        "        e_score = calculate_experience_match(candidate['experience_level'], job['experience_level'])\n",
        "        l_score = calculate_location_match(candidate['preferred_locations'], job['job_location'], candidate['open_to_remote'])\n",
        "        sal_score = calculate_salary_match(candidate.get('desired_salary_min'), job.get('salary'))\n",
        "\n",
        "        # Weighted Total\n",
        "        total = 0.45 * s_score + 0.25 * e_score + 0.15 * l_score + 0.15 * sal_score\n",
        "\n",
        "\n",
        "        results.append({\n",
        "            'job_id': job['job_id'],\n",
        "            'job_title': job['job_title'],\n",
        "            'company': job['company'],\n",
        "            'overall_score': total,\n",
        "            'matching_skills': list(set(candidate['skills_list']) & set(job['skills_list'])),\n",
        "            'missing_skills': list(set(job['skills_list']) - set(candidate['skills_list']))\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).sort_values('overall_score', ascending=False).head(top_k)"
      ],
      "metadata": {
        "id": "Cec2kLod95MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pick a candidate\n",
        "my_cand_id = candidates_df.iloc[0]['candidate_id']\n",
        "\n",
        "# 2. Run the recommender\n",
        "top_jobs = get_recommendations(my_cand_id, filtered_df, candidates_df)\n",
        "\n",
        "# 3. Look at the results\n",
        "print(f\"Top matches for {my_cand_id}:\")\n",
        "display(top_jobs)"
      ],
      "metadata": {
        "id": "fUUnAlkYugbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = len(top_jobs)\n",
        "print(f\"\\n  Top {top_k} Job Recommendations for {my_cand_id}\")\n",
        "print(f\"   Candidate Profile: {candidates_df[candidates_df['candidate_id']==my_cand_id]['experience_level'].values[0]}\")\n",
        "print(f\"   Domain: {candidates_df[candidates_df['candidate_id']==my_cand_id]['domain'].values[0]}\\n\")\n",
        "\n",
        "for idx, row in top_jobs.iterrows():\n",
        "    print(f\"{idx+1}. {row['job_title']} at {row['company']}\")\n",
        "    print(f\"   Match Score: {row['overall_score']:.2%}\")\n",
        "    print(f\"    Matching Skills: {', '.join(row['matching_skills'][:5])}\")\n",
        "    print(f\"    Missing Skills: {', '.join(row['missing_skills'][:3])}\\n\")\n"
      ],
      "metadata": {
        "id": "lh1ejwMAu0nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Colab Filtering\n",
        "- Learns patterns from how candidates interact with jobs to make recommendations"
      ],
      "metadata": {
        "id": "k9-LnpnqHmIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulating user interactions\n",
        "- We will simulate which candidates applied to which jobs and to make it more realistic: candidates apply to job that they match well with"
      ],
      "metadata": {
        "id": "0d8KZV1phIB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_interactions(candidates_df, jobs_df):\n",
        "     #Parse skills if they're strings\n",
        "    if isinstance(candidates_df['skills_list'].iloc[0], str):\n",
        "        candidates_df = candidates_df.copy()\n",
        "        candidates_df['skills_list'] = candidates_df['skills_list'].apply(ast.literal_eval)\n",
        "\n",
        "    if isinstance(jobs_df['skills_list'].iloc[0], str):\n",
        "        jobs_df = jobs_df.copy()\n",
        "        jobs_df['skills_list'] = jobs_df['skills_list'].apply(ast.literal_eval)\n",
        "\n",
        "    # Simulates candidates applying to jobs\n",
        "    interactions = []\n",
        "\n",
        "    sample_candidates = candidates_df.sample(min(500, len(candidates_df)))\n",
        "    sample_jobs = jobs_df.sample(min(2000, len(jobs_df)))\n",
        "\n",
        "    for _, candidate in sample_candidates.iterrows():\n",
        "        n_applications = np.random.randint(5, 15)\n",
        "\n",
        "        scores = []\n",
        "        eligible_jobs = []\n",
        "\n",
        "        #  Filter realistic jobs\n",
        "        for _, job in sample_jobs.iterrows():\n",
        "            # Candidates only apply to jobs with >25% skill match\n",
        "            skill_score = calculate_skill_match_score(\n",
        "                candidate['skills_list'],\n",
        "                job['skills_list']\n",
        "            )\n",
        "\n",
        "            if skill_score < 0.25:\n",
        "                continue  # realism fix\n",
        "\n",
        "            exp_score = calculate_experience_match(\n",
        "                candidate['experience_level'],\n",
        "                job['experience_level']\n",
        "            )\n",
        "\n",
        "            total_score = 0.7 * skill_score + 0.3 * exp_score\n",
        "            scores.append(total_score)\n",
        "            eligible_jobs.append(job)\n",
        "\n",
        "        if len(eligible_jobs) == 0:\n",
        "            continue\n",
        "\n",
        "        # Sample applications\n",
        "        scores = np.array(scores)\n",
        "        probs = scores / scores.sum()\n",
        "\n",
        "        selected_indices = np.random.choice(\n",
        "            len(eligible_jobs),\n",
        "            size=min(n_applications, len(eligible_jobs)),\n",
        "            replace=False,\n",
        "            p=probs\n",
        "        )\n",
        "\n",
        "        #  Generates ratings (1-5 stars) correlated with skill match\n",
        "        for idx in selected_indices:\n",
        "            job = eligible_jobs[idx]\n",
        "\n",
        "            skill_score = calculate_skill_match_score(\n",
        "                candidate['skills_list'],\n",
        "                job['skills_list']\n",
        "            )\n",
        "\n",
        "            rating = np.clip(\n",
        "                1 + 4 * skill_score + np.random.normal(0, 0.3),\n",
        "                1, 5\n",
        "            )\n",
        "\n",
        "            interactions.append({\n",
        "                'candidate_id': candidate['candidate_id'],\n",
        "                'job_id': job['job_id'],\n",
        "                'rating': rating,\n",
        "                'applied': 1\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(interactions)\n"
      ],
      "metadata": {
        "id": "_SahoMF39-gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating candidate-job interactions...\")\n",
        "interactions_df = simulate_interactions(candidates_df, filtered_df)\n",
        "\n",
        "print(f\"\\nInteraction Statistics:\")\n",
        "print(f\"  Total interactions: {len(interactions_df):,}\")\n",
        "print(f\"  Unique candidates: {interactions_df['candidate_id'].nunique()}\")\n",
        "print(f\"  Unique jobs: {interactions_df['job_id'].nunique()}\")\n",
        "print(f\"  Avg applications per candidate: {len(interactions_df) / interactions_df['candidate_id'].nunique():.1f}\")\n",
        "print(f\"  Rating distribution:\\n{interactions_df['rating'].value_counts().sort_index()}\")\n",
        "\n",
        "# Save interactions\n",
        "interactions_df.to_csv('data/processed/interactions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ufH5sOtYwl4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a collaborative filtering model"
      ],
      "metadata": {
        "id": "jKlBl258NvpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "class CollaborativeRecommender:\n",
        "    \"\"\"\n",
        "    Matrix factorization based collaborative filtering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_factors=50):\n",
        "        self.n_factors = n_factors\n",
        "        self.svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.user_to_idx = None\n",
        "        self.job_to_idx = None\n",
        "        self.idx_to_job = None\n",
        "\n",
        "    def fit(self, interactions_df):\n",
        "        \"\"\"\n",
        "        Fit the collaborative filtering model\n",
        "        \"\"\"\n",
        "        print(\" Training collaborative filtering model...\")\n",
        "\n",
        "        # Create user-item matrix\n",
        "        self.user_to_idx = {user: idx for idx, user in\n",
        "                           enumerate(interactions_df['candidate_id'].unique())}\n",
        "        self.job_to_idx = {job: idx for idx, job in\n",
        "                          enumerate(interactions_df['job_id'].unique())}\n",
        "        self.idx_to_job = {idx: job for job, idx in self.job_to_idx.items()}\n",
        "\n",
        "        n_users = len(self.user_to_idx)\n",
        "        n_jobs = len(self.job_to_idx)\n",
        "\n",
        "        print(f\"  Matrix size: {n_users} candidates Ã— {n_jobs} jobs\")\n",
        "\n",
        "        # Create sparse matrix\n",
        "        rows = interactions_df['candidate_id'].map(self.user_to_idx)\n",
        "        cols = interactions_df['job_id'].map(self.job_to_idx)\n",
        "        data = interactions_df['rating'].values\n",
        "\n",
        "        user_item_matrix = csr_matrix((data, (rows, cols)), shape=(n_users, n_jobs))\n",
        "\n",
        "        # Apply SVD\n",
        "        print(f\"  Applying SVD with {self.n_factors} factors...\")\n",
        "        self.user_factors = self.svd.fit_transform(user_item_matrix)\n",
        "        self.item_factors = self.svd.components_.T\n",
        "\n",
        "        print(f\" Model trained!\")\n",
        "        print(f\"  Explained variance: {self.svd.explained_variance_ratio_.sum():.2%}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, candidate_id, job_id):\n",
        "        \"\"\"\n",
        "        Predict rating for candidate-job pair\n",
        "        \"\"\"\n",
        "        if candidate_id not in self.user_to_idx or job_id not in self.job_to_idx:\n",
        "            return 0.0\n",
        "\n",
        "        user_idx = self.user_to_idx[candidate_id]\n",
        "        job_idx = self.job_to_idx[job_id]\n",
        "\n",
        "        prediction = np.dot(self.user_factors[user_idx], self.item_factors[job_idx])\n",
        "        return prediction\n",
        "\n",
        "    def recommend_jobs(self, candidate_id, jobs_df, top_k=10, exclude_applied=None):\n",
        "        \"\"\"\n",
        "        Recommend jobs for a candidate using CF\n",
        "        \"\"\"\n",
        "        if candidate_id not in self.user_to_idx:\n",
        "            return pd.DataFrame()  # Cold start\n",
        "\n",
        "        user_idx = self.user_to_idx[candidate_id]\n",
        "\n",
        "        # Calculate scores for all jobs\n",
        "        scores = np.dot(self.user_factors[user_idx], self.item_factors.T)\n",
        "        scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)\n",
        "\n",
        "        # Get job IDs in order\n",
        "        job_ids = [self.idx_to_job[i] for i in range(len(scores))]\n",
        "\n",
        "        # Create recommendations dataframe\n",
        "        recs = pd.DataFrame({\n",
        "            'job_id': job_ids,\n",
        "            'cf_score': scores\n",
        "        })\n",
        "\n",
        "        # Merge with job details\n",
        "        recs = recs.merge(\n",
        "            jobs_df[['job_id', 'job_title', 'company', 'job_location', 'experience_level']],\n",
        "            on='job_id'\n",
        "        )\n",
        "\n",
        "        # Exclude already applied jobs\n",
        "        if exclude_applied is not None:\n",
        "            recs = recs[~recs['job_id'].isin(exclude_applied)]\n",
        "\n",
        "        # Sort and return top K\n",
        "        return recs.sort_values('cf_score', ascending=False).head(top_k)\n",
        "\n",
        "# Train the model\n",
        "n_factors = min(30, interactions_df['job_id'].nunique() - 1)\n",
        "\n",
        "cf_recommender = CollaborativeRecommender(n_factors=n_factors)\n",
        "cf_recommender.fit(interactions_df)\n",
        "\n",
        "\n",
        "print(\"\\n Collaborative filtering model ready\")\n"
      ],
      "metadata": {
        "id": "xXUA9YaqHx_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Sample Recommendations:\")\n",
        "sample_candidate = candidates_df.sample(1).iloc[0]\n",
        "print(f\"Candidate: {sample_candidate['candidate_id']}\")\n",
        "print(f\"  Experience: {sample_candidate['experience_level']}\")\n",
        "print(f\"  Domain: {sample_candidate['domain']}\")\n",
        "\n",
        "# Get their applied jobs\n",
        "applied_jobs = interactions_df[\n",
        "    interactions_df['candidate_id'] == sample_candidate['candidate_id']\n",
        "]['job_id'].tolist()\n",
        "\n",
        "# Get CF recommendations\n",
        "recs = cf_recommender.recommend_jobs(\n",
        "    sample_candidate['candidate_id'],\n",
        "    filtered_df,\n",
        "    top_k=5,\n",
        "    exclude_applied=applied_jobs\n",
        ")\n",
        "\n",
        "print(\"\\nTop 5 Recommended Jobs:\")\n",
        "print(recs[['job_title', 'company', 'cf_score']].to_string(index=False))\n"
      ],
      "metadata": {
        "id": "AFvXgKJHyOG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "\n",
        "# Save collaborative model\n",
        "with open(\"models/cf_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(cf_recommender, f)\n"
      ],
      "metadata": {
        "id": "ivgJPlhX8jxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid"
      ],
      "metadata": {
        "id": "2cvZKNoxH184"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_minmax(series):\n",
        "    if series.max() == series.min():\n",
        "        return np.zeros(len(series))\n",
        "    return (series - series.min()) / (series.max() - series.min())"
      ],
      "metadata": {
        "id": "GX6aVfOy7tf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_recommend(candidate_id, content_weight=0.6, cf_weight=0.4, top_k=10):\n",
        "    \"\"\"\n",
        "    Combine content-based and collaborative filtering\n",
        "\n",
        "    Parameters:\n",
        "    - content_weight: Weight for content-based score (0-1)\n",
        "    - cf_weight: Weight for collaborative filtering score (0-1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get content-based recommendations\n",
        "    content_recs = get_recommendations(candidate_id, filtered_df, candidates_df, top_k=100)\n",
        "\n",
        "    # Get CF recommendations (if candidate has interactions)\n",
        "    candidate_interactions = interactions_df[\n",
        "        interactions_df['candidate_id'] == candidate_id\n",
        "    ]['job_id'].tolist()\n",
        "\n",
        "    cf_recs = cf_recommender.recommend_jobs(\n",
        "        candidate_id,\n",
        "        filtered_df,\n",
        "        top_k=100,\n",
        "        exclude_applied=candidate_interactions\n",
        "    )\n",
        "\n",
        "    # If no CF recommendations (cold start), use only content-based\n",
        "    if len(cf_recs) == 0:\n",
        "      print(f\" Cold start for {candidate_id} - using content-based only\")\n",
        "\n",
        "      content_recs = content_recs.head(top_k).copy()\n",
        "      content_recs['content_score_norm'] = (\n",
        "        content_recs['overall_score'] / content_recs['overall_score'].max()\n",
        "      )\n",
        "      content_recs['cf_score_norm'] = 0.0\n",
        "      content_recs['hybrid_score'] = content_recs['content_score_norm']\n",
        "\n",
        "      return content_recs\n",
        "\n",
        "\n",
        "    # Merge both recommendations\n",
        "    # Normalize scores to 0-1 range\n",
        "    content_recs['content_score_norm'] = safe_minmax(content_recs['overall_score'])\n",
        "    cf_recs['cf_score_norm'] = safe_minmax(cf_recs['cf_score'])\n",
        "\n",
        "    # Merge on job_title and company (since job_id might differ)\n",
        "    merged = content_recs.merge(\n",
        "        cf_recs[['job_id', 'cf_score_norm']],\n",
        "        on='job_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Fill NaN CF scores with 0 (jobs not in CF model)\n",
        "    merged['cf_score_norm'] = merged['cf_score_norm'].fillna(0)\n",
        "\n",
        "    # Calculate hybrid score\n",
        "    merged['hybrid_score'] = (\n",
        "        content_weight * merged['content_score_norm'] +\n",
        "        cf_weight * merged['cf_score_norm']\n",
        "    )\n",
        "\n",
        "    # Sort by hybrid score\n",
        "    result = merged.sort_values('hybrid_score', ascending=False).head(top_k)\n",
        "\n",
        "    return result[[\n",
        "        'job_id','job_title', 'company', 'hybrid_score',\n",
        "        'content_score_norm', 'cf_score_norm',\n",
        "        'matching_skills', 'missing_skills'\n",
        "    ]]\n",
        "\n",
        "# Test hybrid recommender\n",
        "print(\"=\"*60)\n",
        "print(\"Testing Hybrid Recommender\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_candidate = candidates_df.iloc[0]['candidate_id']\n",
        "print(f\"\\nCandidate: {test_candidate}\")\n",
        "\n",
        "hybrid_recs = hybrid_recommend(test_candidate, content_weight=0.6, cf_weight=0.4, top_k=10)\n",
        "\n",
        "print(\"\\n Top 10 Hybrid Recommendations:\")\n",
        "display(hybrid_recs)\n"
      ],
      "metadata": {
        "id": "gl74PH-8H22W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_recommenders(candidate_id, top_k=5):\n",
        "    \"\"\"\n",
        "    Compare all three recommendation approaches\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Recommendation Comparison for {candidate_id}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    candidate = candidates_df[candidates_df['candidate_id'] == candidate_id].iloc[0]\n",
        "    print(f\"\\nCandidate Profile:\")\n",
        "    print(f\"  Experience: {candidate['experience_level']} ({candidate['experience_years']} years)\")\n",
        "    print(f\"  Top Skills: {candidate['skills_list'][:5]}\")\n",
        "    print(f\"  Preferred Locations: {candidate['preferred_locations']}\")\n",
        "\n",
        "    # 1. Content-Based\n",
        "    print(\"\\n\" + \"â”€\"*80)\n",
        "    print(\" CONTENT-BASED RECOMMENDATIONS\")\n",
        "    print(\"â”€\"*80)\n",
        "    content_recs = get_recommendations(candidate_id, filtered_df, candidates_df, top_k=top_k)\n",
        "    for i, (_, row) in enumerate(content_recs.iterrows(), 1):\n",
        "        print(f\"{i}. {row['job_title']} at {row['company']}\")\n",
        "        print(f\"   Score: {row['overall_score']:.3f}\")\n",
        "        print(f\"   Matching: {len(row['matching_skills'])} skills | Missing: {len(row['missing_skills'])} skills\")\n",
        "\n",
        "    # 2. Collaborative Filtering\n",
        "    print(\"\\n\" + \"â”€\"*80)\n",
        "    print(\" COLLABORATIVE FILTERING RECOMMENDATIONS\")\n",
        "    print(\"â”€\"*80)\n",
        "\n",
        "    candidate_interactions = interactions_df[\n",
        "        interactions_df['candidate_id'] == candidate_id\n",
        "    ]['job_id'].tolist()\n",
        "\n",
        "    cf_recs = cf_recommender.recommend_jobs(\n",
        "        candidate_id,\n",
        "        filtered_df,\n",
        "        top_k=top_k,\n",
        "        exclude_applied=candidate_interactions\n",
        "    )\n",
        "\n",
        "    if len(cf_recs) > 0:\n",
        "        for i, (_, row) in enumerate(cf_recs.iterrows(), 1):\n",
        "            print(f\"{i}. {row['job_title']} at {row['company']}\")\n",
        "            print(f\"   CF Score: {row['cf_score']:.3f}\")\n",
        "    else:\n",
        "        print(\" No CF recommendations (cold start)\")\n",
        "\n",
        "    # 3. Hybrid\n",
        "    print(\"\\n\" + \"â”€\"*80)\n",
        "    print(\" HYBRID RECOMMENDATIONS (Best of Both)\")\n",
        "    print(\"â”€\"*80)\n",
        "    hybrid_recs = hybrid_recommend(candidate_id, top_k=top_k)\n",
        "    for i, (_, row) in enumerate(hybrid_recs.iterrows(), 1):\n",
        "        print(f\"{i}. {row['job_title']} at {row['company']}\")\n",
        "        print(f\"   Hybrid: {row['hybrid_score']:.3f} | Content: {row['content_score_norm']:.3f} | CF: {row['cf_score_norm']:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Run comparison\n",
        "test_candidate = candidates_df.iloc[5]['candidate_id']  # Try different candidates\n",
        "compare_recommenders(test_candidate, top_k=5)\n"
      ],
      "metadata": {
        "id": "x2JMtkmQH6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
        "\n",
        "def evaluate_recommender(interactions_test, recommender_func, k=10):\n",
        "    \"\"\"\n",
        "    Evaluate recommendation quality\n",
        "\n",
        "    Metrics:\n",
        "    - Precision@K: What % of recommendations are relevant?\n",
        "    - Recall@K: What % of relevant items are recommended?\n",
        "    - NDCG@K: Ranking quality (higher rated items should rank higher)\n",
        "    \"\"\"\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "\n",
        "    # Get unique candidates from test set\n",
        "    test_candidates = interactions_test['candidate_id'].unique()\n",
        "\n",
        "    for candidate_id in test_candidates[:100]:  # Sample 100\n",
        "        # Get actual relevant jobs (\n",
        "        actual_relevant = interactions_test[\n",
        "            (interactions_test['candidate_id'] == candidate_id) &\n",
        "            (interactions_test['rating'] >= 4.0)\n",
        "        ]['job_id'].tolist()\n",
        "\n",
        "        if len(actual_relevant) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get recommendations\n",
        "        try:\n",
        "            recs = recommender_func(candidate_id, top_k=k)\n",
        "            if len(recs) == 0:\n",
        "                continue\n",
        "\n",
        "            recommended_jobs = recs['job_id'].tolist()\n",
        "\n",
        "            # Calculate metrics\n",
        "            hits = len(set(recommended_jobs) & set(actual_relevant))\n",
        "\n",
        "            precision = hits / k if k > 0 else 0\n",
        "            recall = hits / len(actual_relevant) if len(actual_relevant) > 0 else 0\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        'precision@k': np.mean(precisions) if precisions else 0,\n",
        "        'recall@k': np.mean(recalls) if recalls else 0,\n",
        "        'coverage': len(precisions) / len(test_candidates)\n",
        "    }\n",
        "\n",
        "# Split interactions into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_interactions, test_interactions = train_test_split(\n",
        "    interactions_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\" Evaluation Results (on test set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate content-based\n",
        "print(\"\\n1. Content-Based Recommender:\")\n",
        "content_metrics = evaluate_recommender(\n",
        "    test_interactions,\n",
        "    lambda cid, top_k: get_recommendations(cid, filtered_df, candidates_df, top_k),\n",
        "    k=10\n",
        ")\n",
        "for metric, value in content_metrics.items():\n",
        "    print(f\"   {metric}: {value:.4f}\")\n",
        "\n",
        "# Evaluate hybrid\n",
        "print(\"\\n2. Hybrid Recommender:\")\n",
        "hybrid_metrics = evaluate_recommender(\n",
        "    test_interactions,\n",
        "    lambda cid, top_k: hybrid_recommend(cid, top_k=top_k),\n",
        "    k=10\n",
        ")\n",
        "for metric, value in hybrid_metrics.items():\n",
        "    print(f\"   {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n Evaluation complete!\")\n"
      ],
      "metadata": {
        "id": "HFrKeXLcH_PJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}